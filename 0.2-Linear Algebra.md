[Linear Programming](29-Operations Research for Management.md#Linear Programming)

In this course, I mainly studied vectors and matrices, solving systems of linear equations, matrix rank and invertibility, eigenvalues and eigenvectors, and the basic concepts of vector spaces. 

These topics form the core framework of linear algebra and help us understand and handle linear relationships among multiple variables, especially in applications such as data modeling, engineering computations, and optimization problems. 

## Vectors and Matrices

Vectors and matrices are the most fundamental concepts in linear algebra. We can think of a **vector as an arrow with direction and length.** 

- For example, in a 2D plane, **the point (3, 4) can be seen as a vector pointing from the origin to that point**. Its length is 5 (calculated by the Pythagorean theorem: √(3²+4²) = 5), and its direction is to the upper right. In real-world problems, vectors can represent many things—for instance, the price and sales volume of a product can form a 2D vector like (price, quantity sold), such as (20 yuan, 150 units). 

**A matrix, on the other hand, is a rectangular array of numbers.** 

- For example, a 2×3 matrix has two rows and three columns, like this: first row [1, 2, 3], second row [4, 5, 6]. 

**Matrices can represent multiple vectors or describe a "transformation"—such as rotating, stretching, or compressing a vector.** 

- For example, if we have a matrix A = [[2, 0], [0, 1]], and it acts on a vector [x, y], the result is [2x, y]. This means the x-component is doubled while the y-component stays the same—like stretching a square horizontally into a rectangle. Such transformations are very common in computer graphics and data analysis.

## Linear Equations

Solving systems of linear equations is a major application in this course. 

- For example, suppose we have three unknowns: x, y, z, and three equations: x + y + z = 6, 2x - y + z = 3, and x + 2y - z = 2. 

Such problems are common in economic models and engineering design—like determining the optimal mix of three raw materials to meet cost and performance requirements. 

We can represent this system using matrices: the coefficients form a 3×3 matrix A, the unknowns form a column vector x, and the results on the right-hand side form another column vector b, turning the entire system into the matrix equation A·x = b. 

Then we use **Gaussian elimination**—performing row operations (such as swapping rows, multiplying a row by a scalar, or adding one row to another) to transform the coefficient matrix into an upper triangular matrix. Finally, we solve for each variable step by step through back-substitution. 

- In the example above, after elimination, we get x=1, y=2, z=3. 

The key advantage of this method is that it systematically simplifies the problem, avoids messy manual calculations, and is especially suitable for implementation on computers.

## Rank and Invertibility

The rank and invertibility of a matrix help us determine whether a system of equations has a unique solution. 

The "**rank**" of a matrix is <u>the maximum number of linearly independent rows or columns</u>. 

For example, in a 3×3 matrix, if none of the three rows can be expressed as a combination of the other two through addition or scalar multiplication, the rank is 3. But if the third row equals the sum of the first two (e.g., row3 = row1 + row2), then the rank is 2. <u>Rank is important because it tells us how much "independent information" the system contains</u>. 

1. If the rank of the coefficient matrix equals the number of unknowns and also equals the rank of the augmented matrix (the coefficient matrix with the constants on the right appended as an extra column), then the system has a unique solution. 
2. If the rank is less than the number of unknowns, there may be infinitely many solutions.
3. If the ranks of the coefficient and augmented matrices differ, there is no solution. 

An "invertible matrix" is like the reciprocal of a number—for example, the reciprocal of 2 is 1/2 because 2×(1/2)=1. For a square matrix A, if there exists another matrix B such that A·B = B·A = I (where I is the identity matrix—a matrix with 1s on the diagonal and 0s elsewhere, e.g., the 2×2 identity matrix is [[1,0],[0,1]]), then B is the inverse of A. 

**A matrix is only invertible if its rank equals its dimension** (e.g., a 3×3 matrix must have rank 3). Invertible matrices are very useful in solving A·x = b because we can then directly write x = A⁻¹·b, just like solving 2x=6 by dividing both sides by 2 to get x=3. **Which means many good properties.**

## Eigenvalues and Eigenvectors

Eigenvalues and eigenvectors are relatively abstract but extremely important concepts in linear algebra. 

**Simply put, for a square matrix A, if there exists a non-zero vector v and a scalar λ such that A·v = λ·v, then v is an eigenvector of A and λ is the corresponding eigenvalue. This equation means that when matrix A acts on vector v, the result is simply v stretched or compressed by a factor of λ, without changing its direction (or reversed if λ is negative).** 

- For example, take matrix A = [[4, 1], [2, 3]]. One of its eigenvectors is [1, 1], with eigenvalue 5, because A·[1,1] = [5,5] = 5·[1,1]. This shows that **the matrix’s effect along this direction is just a 5-fold stretch**. 

Eigenvalues and eigenvectors are very useful in data analysis. For instance, Principal Component Analysis (PCA) **finds the eigenvectors of a covariance matrix to identify the most important directions of variation in data, enabling dimensionality reduction. **In facial recognition, eigenvectors can represent "eigenfaces," allowing us to represent complex images with fewer data points.

## Vector Spaces

Vector spaces are the theoretical foundation of linear algebra. A vector space is a set of vectors that is closed under two operations: **vector addition and scalar multiplication**—meaning that adding two vectors or multiplying a vector by a number always results in another vector in the same set. 

- For example, the 2D plane ℝ² is a vector space—adding any two 2D vectors or multiplying one by a scalar always yields another 2D vector. 

**A "basis" of a vector space** is a special set of vectors that are linearly independent and can be combined (through linear combinations) to represent any vector in the space. In ℝ², the standard basis is [1,0] and [0,1], because any vector [a,b] can be written as a·[1,0] + b·[0,1]. The number of vectors in a basis is the "dimension" of the space—for example, ℝ² is 2-dimensional. 

- Understanding vector spaces helps us think about data structures in higher dimensions. In machine learning, for instance, each data sample might be a high-dimensional vector (e.g., with 100 features), and all samples together form a high-dimensional vector space, where algorithms search for patterns or decision boundaries.