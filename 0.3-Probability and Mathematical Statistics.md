[Sampling inspection theory](32-Quality Management.md#Sampling inspection theory)

In this course, I mainly studied the fundamentals of probability theory, random variables and their distributions, parameter estimation, and hypothesis testing. These topics form the core tools for understanding and analyzing an uncertain world, which are essential in engineering decisions, financial risk assessment, quality control, and many other fields.

Probability and Statistics taught me how to use mathematical tools to handle uncertainty in real life. From understanding basic probability rules, to modeling random phenomena, to making scientific inferences based on data, this course trained me to speak with data rather than rely on intuition. These skills not only support my major studies but also lay a solid foundation in quantitative analysis for my future graduate studies in Germany.

## Probability Rules

The foundation of probability theory is the starting point of the entire course, teaching us how to use mathematical language to describe "likelihood." 

- For example, in real life, we often encounter uncertain events—such as whether it will rain tomorrow, whether a mechanical part will fail within a year, or how many students will pass an exam. 

**Probability theory helps us quantify these uncertainties**. We started with basic concepts like the sample space, which refers to the set of all possible outcomes. 

- For instance, if I flip a coin, the sample space is {Heads, Tails}; if I roll a six-sided die, the sample space is {1, 2, 3, 4, 5, 6}. Then we define an event, such as "rolling an even number," which includes the outcomes {2, 4, 6}. 

Probability is a number between 0 and 1 that represents how likely an event is to occur. 

- For example, the probability of rolling an even number is 3/6 = 0.5. 

We also learned basic probability rules: **the addition rule**, which says that probabilities of mutually exclusive events (events that cannot happen at the same time) can be added; and **the multiplication rule**, which says that for independent events (where one does not affect the other), the joint probability is the product of individual probabilities. 

- For example, the probability of getting heads twice in a row when flipping a fair coin is 0.5 × 0.5 = 0.25. These seemingly simple rules form the foundation for more complex models later on.

## Random Variables and Their Distributions

**A random variable is actually not really a "variable" but a "function"—it maps each possible outcome to a numerical value.** 

- For example, using the dice-rolling case again: each roll result is a random variable X, which can take integer values from 1 to 6. If we're dealing with continuous data, such as measuring the lifespan of a light bulb, then that lifespan is a continuous random variable, which could be any positive real number. 

Once we have a random variable, we can study its **distribution**—how likely different values are to occur. 

- One of the most common distributions is the normal distribution, which looks like a bell-shaped curve—high in the middle and tapering off on both sides. 

Many natural phenomena approximately follow a **normal distribution**, such as adult heights, student test scores, or the dimensions of manufactured parts. 

- For example, suppose a brand of light bulbs has an average lifespan of 800 hours with a standard deviation of 50 hours. This means most bulbs last between 750 and 850 hours—about 68% fall within this range (mean ± 1 standard deviation), and about 95% fall between 700 and 900 hours (mean ± 2 standard deviations). 

We use a **probability density function** to describe this distribution and can calculate probabilities over intervals—for instance, 

- "What's the chance that a bulb lasts more than 900 hours?" Using a standardization transformation (Z-score), we can look up the standard normal table and find the answer is approximately 2.3%. Mastering these distribution models allows us to predict system behavior and plan ahead.

## Parameter Estimation

Then comes parameter estimation, which deals with how to **"guess" the characteristics of a population when we don’t know the true situation, by using sample data.** 

- For example, I want to know how much time college students across the country spend on their phones daily, but I can't survey every single student. So instead, I randomly select a sample of 1,000 students and calculate their average—say, 3.2 hours. This 3.2 hours is a point estimate—a guess of the true population mean. But the problem is, how accurate is this guess? 

That’s why we introduce **interval estimation**. 

- For instance, I can construct a 95% confidence interval and say, “I’m 95% confident that the average daily phone usage among college students nationwide is between 3.0 and 3.4 hours.” The width of this interval depends on the sample size and the variability in the data—larger samples lead to narrower intervals, meaning more precise estimates. 

**It’s like using a telescope to observe stars: the longer you observe (the more data you collect), the clearer the image becomes.** We also discuss criteria for good estimators, such as **unbiasedness**—the estimator’s average value over many samples should equal the true parameter, not systematically too high or too low—and efficiency—among unbiased estimators, the one with smaller variance is more stable and reliable. These ideas are crucial in practice; for example, when companies conduct market research, they can’t survey every consumer, so they must rely on sound sampling and estimation methods to make informed decisions.

## Hypothesis Testing

Finally, hypothesis testing is one of the most commonly used methods in statistical inference, **used to judge whether a certain claim is supported by evidence.** 

- For example, a pharmaceutical company claims that their new blood pressure drug reduces systolic pressure by at least 20 mmHg on average. 

How do we verify this claim? That’s where **hypothesis testing** comes in. 

> First, we set up a null hypothesis (H₀): the drug reduces blood pressure by 20 mmHg or less on average; and an alternative hypothesis (H₁): the drug reduces it by more than 20 mmHg. 
>
> Then we recruit 50 patients to try the drug, record their blood pressure changes before and after treatment, and compute the sample mean—say, it shows an average reduction of 22 mmHg. 
>
> Now we need to decide: is 22 large enough to reject the conservative assumption (H₀)? We answer this by calculating the p-value. The p-value represents the probability of observing our current result—or something more extreme—if the null hypothesis were actually true. 

Suppose we calculate a p-value of 0.03. This means that if the drug truly reduced pressure by no more than 20 mmHg, there would only be a 3% chance of seeing an average drop of 22 mmHg or greater. Since this probability is small, we compare it to a pre-set significance level α (commonly 0.05). If the p-value is less than α, we reject the null hypothesis. In this case, 0.03 < 0.05, so we have sufficient evidence to reject H₀ and support the company’s claim. This process is like a court trial: the null hypothesis is like “the defendant is innocent,” and we only convict if there is strong evidence (a very small p-value). Hypothesis testing is widely used in quality control, medical trials, policy evaluation, and many other areas.
