In this course, I mainly studied four core topics: the fundamental concepts and technical architecture of big data, data preprocessing and cleaning, the application of machine learning in big data, and big data visualization. 

The focus of this course is to help us understand how to extract valuable information from massive and complex datasets and use that information to support decision-making. 

For example, in industries like e-commerce, finance, and logistics, companies generate huge amounts of user behavior and transaction data every day. If this data can be effectively utilized, businesses can better understand their customers, optimize services, and even predict future trends. Through theoretical learning and hands-on case studies, we mastered a complete workflow and tools for processing and analyzing big data.

## Big Data

The industry typically uses five "V"s to describe the characteristics of big data: Volume (huge amount of data), Velocity (high speed of data generation and processing), Variety (diverse data types), Veracity (data accuracy or reliability), and Value (low value density). 

- For example, on a social media platform like Weibo, millions of new posts, comments, and shares can be generated every minute—this illustrates **Volume and Velocity**. 
- These data include not only text, but also images, videos, and geolocation information, which reflects **Variety**. 
- However, the truly useful information among them might only be a small fraction, such as the trend of discussion around a popular topic—this shows **low Value density**. 

To handle such data, traditional relational databases (like MySQL) are insufficient because they are slow and hard to scale. Therefore, we learned about distributed systems like Hadoop, which can break a large task into smaller ones and distribute them across hundreds or thousands of ordinary computers to process simultaneously—similar to a factory dividing a production line into multiple parallel workgroups, greatly improving efficiency. The core components of Hadoop are HDFS (Hadoop Distributed File System) for data storage and MapReduce for data processing. Later, we also studied Spark, which is faster than Hadoop because it keeps more data in memory rather than repeatedly reading and writing to disk—like using high-speed cache instead of a slow hard drive.

## Data preprocessing and cleaning

Data preprocessing and cleaning is a crucial step we must take before actual analysis, because real-world data is often "messy." 

For instance, we once analyzed an operational dataset from a bike-sharing service, which included information like ride duration, start and end locations, and user age for each bike. But we found that some ride durations were negative, like -15 minutes, which is clearly impossible; some user ages were recorded as 0 or 200 years old, likely due to input errors or unmodified system defaults; and some records had start or end coordinates as (0,0), which doesn’t exist on a real map—these are invalid locations. 

If such erroneous data isn’t handled, it can severely affect the accuracy of later analysis. So we learned how to identify and deal with these issues: for example, **using statistical methods to detect outliers and then deciding whether to delete, correct, or keep them based on context**; for missing data, like a user’s age not being filled in, we could fill it with the average age of users from the same region and gender (this is called mean imputation), or use more sophisticated algorithms to predict it. 

We also learned about **data normalization**, such as converting different units (like kilometers and miles) into the same unit, or scaling numerical values to a range between 0 and 1, so that features with different scales can participate fairly in calculations. This process may sound boring, but it’s essential for ensuring the reliability of analysis results. Studies show that in a typical data analysis project, data cleaning and preprocessing can take up more than 60% of the total workload.

## The application of machine learning

We focused on two main methods: supervised learning and unsupervised learning. 

**Supervised learning** is like a teacher training a student—**we provide the algorithm with training data that includes correct answers, so it can learn to predict outputs from inputs.** 

- For example, we used historical order data from an e-commerce website to predict whether a user will purchase a certain product. The data includes features like user age, browsing duration, whether the product was favorited, past purchase count, and a label indicating whether the purchase actually happened (yes or no). We trained this data using models like Logistic Regression or Decision Tree, and the model automatically learns which factors are more important. For instance, the model might find that "browsing duration over 5 minutes" and "having previously purchased similar products" are key indicators of a purchase. Once trained, we can use this model to predict the behavior of new users, helping the website make personalized recommendations. 

**Unsupervised learning**, on the other hand, **has no "correct answers"—the algorithm must discover patterns in the data on its own.** 

- For example, we used the K-means clustering algorithm to analyze GPS trajectory data from taxis in a city, grouping similar driving routes into categories. We found that the data naturally formed five clusters: one group representing high-frequency routes from the airport to downtown, another group showing taxis cruising around entertainment districts at night, and a third group corresponding to commuting routes between residential and business areas during rush hours. These insights can help taxi companies optimize vehicle dispatching or assist urban planners in understanding traffic flow patterns.

## Big Data Visualization

Finally, big data visualization helps us **present complex analysis results in an intuitive way** so that non-technical people can quickly understand them. We learned how to use tools like Tableau and Python’s Matplotlib library to create charts. 

- For example, when analyzing sales data from an e-commerce platform, we found that user purchasing preferences varied greatly across regions. If we only showed sales numbers for each province in a table, a manager might struggle to spot patterns. But when we overlaid the sales data onto a map of China using a geographic heat map—where darker colors represent higher sales—it became immediately clear that the eastern and southern regions were the main sales drivers. 

- Another example: we used a dynamic line chart to show the daily sales of a smartphone over the first 30 days after launch. We could see that sales peaked on the launch day, then gradually declined, but showed two small spikes on day 7 and day 14. By checking marketing records, we found those were exactly the days of time-limited discount events on the e-commerce platform. Such visualizations not only help us discover patterns more easily but also make our presentations more persuasive when reporting to decision-makers.